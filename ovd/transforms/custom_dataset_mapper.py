import copy
import numpy as np
import torch
from detectron2.config import configurable
from detectron2.data import detection_utils as utils
from detectron2.data import transforms as T
from detectron2.data.dataset_mapper import DatasetMapper
from .custom_build_augmentation import build_custom_augmentation
import pickle
from utils.util import check_image_size

__all__ = ["CustomDatasetMapper", "CustomDatasetMapperMix"]


class CustomDatasetMapper(DatasetMapper):
    @configurable
    def __init__(self, is_train: bool,
                 distillation=False,
                 rkd_feat_path='',
                 num_distil_prop=5,
                 vlm_train_des_path='',
                 **kwargs):
        """
        add proposals for distillation
        Args:
            is_train: whether it's used in training or inference
            distillation: whether to use region-based-knowledge distillation
            proposal_path: path of dir containing pesudo-proposals
            num_distil_prop: number of proposals to consider from pseudo proposals
        """
        self.distillation = distillation
        self.rkd_feat_path = rkd_feat_path
        self.num_distil_prop = num_distil_prop
        self.vlm_train_des_path = vlm_train_des_path
        super().__init__(is_train, **kwargs)

    @classmethod
    def from_config(cls, cfg, is_train: bool = True):
        ret = super().from_config(cfg, is_train)
        ret.update({
            "rkd_feat_path": cfg.MODEL.RKD_FEAT_PATH,
            "distillation": cfg.MODEL.DISTILLATION,
            "num_distil_prop": cfg.MODEL.NUM_DISTIL_PROP,
            "vlm_train_des_path": cfg.MODEL.VLM_TRAIN_DES_PATH,
        })
        return ret

    def __call__(self, dataset_dict):
        """
        include pseudo distillation embeddings
        """
        dataset_dict = copy.deepcopy(dataset_dict)
        image = utils.read_image(dataset_dict["file_name"], format=self.image_format)
        dataset_dict["width"], dataset_dict["height"] = image.shape[1], image.shape[0]

        sem_seg_gt = None

        aug_input = T.AugInput(image, sem_seg=sem_seg_gt)
        transforms = self.augmentations(aug_input)
        image, sem_seg_gt = aug_input.image, aug_input.sem_seg

        image_shape = image.shape[:2]  # h, w

        image_name = dataset_dict["file_name"].split('.')[0].split('/')[-1]
        # Loading DES features
        if len(self.vlm_train_des_path) > 0:
            des_file = f'{self.vlm_train_des_path}/{image_name}.pkl'
            with open(des_file, "rb") as c:
                des_feats = pickle.load(c)
                des_feats = (
                    torch.tensor(transforms.apply_box(np.array(des_feats[0])).clip(min=0), dtype=torch.float32),
                    des_feats[1])
            dataset_dict["des_feats"] = des_feats
            dataset_dict['transforms'] = transforms


        # Modification made for MViT
        # Loading CLIP features for RKD (generated using MAVL proposals) with image in dataloader
        if self.distillation and self.is_train and len(self.rkd_feat_path) > 0:
            image_name = dataset_dict["file_name"].split('.')[0].split('/')[-1]
            # load predictions with dataloader only in training
            clip_features_file = f'{self.rkd_feat_path}/{image_name}.pkl'
            with open(clip_features_file, "rb") as c:
                distill_feats = pickle.load(c)
            # process proposal boxes for distillation
            top_n = self.num_distil_prop
            region_boxes = []
            clip_embeds = []
            for p in range(len(distill_feats)):
                box = transforms.apply_box(np.array([distill_feats[p][0]]))[0].clip(min=0).tolist()
                box = np.minimum(box, list(image_shape + image_shape)[::-1])
                region_boxes.append(box)
                clip_embeds.append(distill_feats[p][1])
            # select n based on num features to distill
            region_boxes = region_boxes[0: top_n]
            clip_embeds = clip_embeds[0: top_n]
            region_boxes = torch.tensor(np.array(region_boxes))
            clip_embeds = torch.cat(clip_embeds, 0)
            processed_distill_feats = (region_boxes, clip_embeds)
            dataset_dict["distill_feats"] = processed_distill_feats

        dataset_dict["image"] = torch.as_tensor(np.ascontiguousarray(image.transpose(2, 0, 1)))
        if sem_seg_gt is not None:
            dataset_dict["sem_seg"] = torch.as_tensor(sem_seg_gt.astype("long"))

        if not self.is_train:
            dataset_dict.pop("annotations", None)
            dataset_dict.pop("sem_seg_file_name", None)
            return dataset_dict

        if "annotations" in dataset_dict:
            self._transform_annotations(dataset_dict, transforms, image_shape)

        return dataset_dict


class CustomDatasetMapperMix(DatasetMapper):
    @configurable
    def __init__(self, is_train: bool,
                 with_ann_type=False,
                 dataset_ann=[],
                 use_diff_bs_size=False,
                 dataset_augs=[],
                 pis_proposal_path='',
                 pis_topk_per_class=1,
                 rkd_feat_path='',
                 rkd_ils_feath_path='',
                 distillation=False,
                 num_distil_prop=5,
                 vlm_train_des_path='',
                 **kwargs):
        """
        add image labels
        """
        self.with_ann_type = with_ann_type
        self.dataset_ann = dataset_ann
        self.use_diff_bs_size = use_diff_bs_size
        if self.use_diff_bs_size and is_train:
            self.dataset_augs = [T.AugmentationList(x) for x in dataset_augs]
        self.ann_type = 'box'
        self.pis_proposal_path = pis_proposal_path
        self.pis_topk_per_class = int(pis_topk_per_class)
        self.rkd_feat_path = rkd_feat_path
        self.rkd_ils_feath_path = rkd_ils_feath_path
        self.distillation = distillation
        self.num_distil_prop = num_distil_prop
        super().__init__(is_train, **kwargs)
        self.vlm_train_des_path = vlm_train_des_path

    @classmethod
    def from_config(cls, cfg, is_train: bool = True):
        ret = super().from_config(cfg, is_train)
        ret.update({
            'with_ann_type': cfg.WITH_IMAGE_LABELS,
            'dataset_ann': cfg.DATALOADER.DATASET_ANN,
            'use_diff_bs_size': cfg.DATALOADER.USE_DIFF_BS_SIZE,
            'pis_proposal_path': cfg.MODEL.PIS_PROP_PATH,
            'pis_topk_per_class': cfg.MODEL.ROI_BOX_HEAD.PIS_TOPK_PER_CLASS,
            'rkd_feat_path': cfg.MODEL.RKD_FEAT_PATH,
            'rkd_ils_feath_path': cfg.MODEL.RKD_ILS_FEAT_PATH,
            'distillation': cfg.MODEL.DISTILLATION,
            'num_distil_prop': cfg.MODEL.NUM_DISTIL_PROP,
            "vlm_train_des_path": cfg.MODEL.VLM_TRAIN_DES_PATH,

        })
        if ret['use_diff_bs_size'] and is_train:
            assert cfg.INPUT.CUSTOM_AUG == 'ResizeShortestEdge'
            min_sizes = cfg.DATALOADER.DATASET_MIN_SIZES
            max_sizes = cfg.DATALOADER.DATASET_MAX_SIZES
            ret['dataset_augs'] = [
                build_custom_augmentation(
                    cfg, True, min_size=mi, max_size=ma) \
                for mi, ma in zip(min_sizes, max_sizes)]
        else:
            ret['dataset_augs'] = []

        return ret

    def __call__(self, dataset_dict):
        """
        include image labels
        """
        dataset_dict = copy.deepcopy(dataset_dict)
        if 'file_name' in dataset_dict:
            ori_image = utils.read_image(
                dataset_dict["file_name"], format=self.image_format)
        check_image_size(dataset_dict, ori_image)

        sem_seg_gt = None
        aug_input = T.AugInput(copy.deepcopy(ori_image), sem_seg=sem_seg_gt)
        if self.use_diff_bs_size and self.is_train:
            self.ann_type = 'box' if dataset_dict['dataset_source'] == 0 else 'image'
            transforms = \
                self.dataset_augs[dataset_dict['dataset_source']](aug_input)
        else:
            transforms = self.augmentations(aug_input)
        image, sem_seg_gt = aug_input.image, aug_input.sem_seg

        image_shape = image.shape[:2]  # h, w

        # Loading MViT proposals with image in dataloader
        if self.ann_type == 'image' and self.is_train and len(self.pis_proposal_path) > 0:
            catid2contid = dataset_dict["catid2contid"]
            image_name = dataset_dict["file_name"].split('.')[0].split('/')[-1]
            proposal_file = f'{self.pis_proposal_path}/{image_name}.pkl'
            with open(proposal_file, "rb") as f:
                detections = pickle.load(f)
                # 删除空的检测结果
                detections = {k: v for k, v in detections.items() if len(v[0]) > 0 and len(v[1]) > 0}
            boxes = []
            probas = []
            ordered_targets = []
            for raw_key, (box_list, prob_list) in detections.items():
                if raw_key == "salient":
                    continue
                label_id = catid2contid.get(raw_key, None)
                if label_id is None:
                    continue
                prob_arr = np.array(prob_list, dtype=np.float32)
                if prob_arr.size == 0:
                    continue
                # Keep top-k proposals per class to enable MIL over multiple candidates.
                topk = min(max(self.pis_topk_per_class, 1), prob_arr.size)
                top_inds = np.argsort(-prob_arr)[:topk]
                for ind in top_inds:
                    box = transforms.apply_box(np.array([box_list[ind]]))[0].clip(min=0).tolist()
                    box = np.minimum(box, list(image_shape + image_shape)[::-1])
                    score_val = float(prob_arr[ind])
                    boxes.append(box)
                    probas.append(score_val)
                    ordered_targets.append((int(label_id), box, score_val))
            dataset_dict["cls_specific_props"] = torch.tensor(np.array(boxes))
            dataset_dict["cls_specific_scores"] = torch.tensor(np.array(probas))
            dataset_dict["cls_specific_target_props"] = ordered_targets

        image_name = dataset_dict["file_name"].split('.')[0].split('/')[-1]
        # Loading DES features
        if len(self.vlm_train_des_path) > 0:
            des_file = f'{self.vlm_train_des_path}/{image_name}.pkl'
            with open(des_file, "rb") as c:
                des_feats = pickle.load(c)
                des_feats = (
                    torch.tensor(transforms.apply_box(np.array(des_feats[0])).clip(min=0), dtype=torch.float32),
                    des_feats[1])
            dataset_dict["des_feats"] = des_feats
            dataset_dict['transforms'] = transforms

        # Loading RKD features
        if self.distillation and self.is_train and len(self.rkd_feat_path) > 0:
            if len(self.rkd_ils_feath_path) > 0 and self.ann_type == 'image':  # trick to handle the case IMAGENET-LVIS
                folder_name = image_name.split("_")[0]
                clip_features_file = f'{self.rkd_ils_feath_path}/{folder_name}/{image_name}.pkl'
            else:
                clip_features_file = f'{self.rkd_feat_path}/{image_name}.pkl'
            with open(clip_features_file, "rb") as c:
                distill_feats = pickle.load(c)
            # process proposal boxes for distillation
            region_boxes = []
            clip_embeds = []
            top_n = self.num_distil_prop
            for p in range(len(distill_feats)):
                box = transforms.apply_box(np.array([distill_feats[p][0]]))[0].clip(min=0).tolist()
                box = np.minimum(box, list(image_shape + image_shape)[::-1])
                region_boxes.append(box)
                clip_embeds.append(distill_feats[p][1])
            # select n based on num features to distill
            region_boxes = region_boxes[0: top_n]
            clip_embeds = clip_embeds[0: top_n]
            region_boxes = torch.tensor(np.array(region_boxes))
            clip_embeds = torch.cat(clip_embeds, 0)
            processed_distill_feats = (region_boxes, clip_embeds)
            dataset_dict["distill_feats"] = processed_distill_feats

        dataset_dict["image"] = torch.as_tensor(
            np.ascontiguousarray(image.transpose(2, 0, 1)))

        if sem_seg_gt is not None:
            dataset_dict["sem_seg"] = torch.as_tensor(sem_seg_gt.astype("long"))

        if not self.is_train:
            dataset_dict.pop("annotations", None)
            dataset_dict.pop("sem_seg_file_name", None)
            return dataset_dict

        if "annotations" in dataset_dict:
            for anno in dataset_dict["annotations"]:
                if not self.use_instance_mask:
                    anno.pop("segmentation", None)
                if not self.use_keypoint:
                    anno.pop("keypoints", None)

            all_annos = [
                (utils.transform_instance_annotations(
                    obj, transforms, image_shape,
                    keypoint_hflip_indices=self.keypoint_hflip_indices,
                ), obj.get("iscrowd", 0))
                for obj in dataset_dict.pop("annotations")
            ]
            annos = [ann[0] for ann in all_annos if ann[1] == 0]
            instances = utils.annotations_to_instances(
                annos, image_shape, mask_format=self.instance_mask_format
            )

            del all_annos
            if self.recompute_boxes:
                instances.gt_boxes = instances.gt_masks.get_bounding_boxes()
            dataset_dict["instances"] = utils.filter_empty_instances(instances)
        if self.with_ann_type:
            dataset_dict["pos_category_ids"] = dataset_dict.get(
                'pos_category_ids', [])
            dataset_dict["ann_type"] = \
                self.dataset_ann[dataset_dict['dataset_source']]
        return dataset_dict


class CustomDatasetEvalMapper(DatasetMapper):

    @configurable
    def __init__(self, is_train: bool,
                 vlm_eval_des_path='',
                 **kwargs):

        super().__init__(is_train, **kwargs)
        self.vlm_eval_des_path = vlm_eval_des_path

    @classmethod
    def from_config(cls, cfg, is_train: bool = False):
        ret = super().from_config(cfg, is_train)
        ret.update({
            "is_train": is_train,
            "vlm_eval_des_path": cfg.MODEL.VLM_EVAL_DES_PATH,
        })
        return ret

    def __call__(self, dataset_dict):
        """
        include pseudo distillation embeddings
        """
        image_name = dataset_dict["file_name"].split('.')[0].split('/')[-1]

        dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below
        # USER: Write your own image loading if it's not from a file
        image = utils.read_image(dataset_dict["file_name"], format=self.image_format)
        utils.check_image_size(dataset_dict, image)
        # Loading DES features
        if len(self.vlm_eval_des_path) > 0:
            des_file = f'{self.vlm_eval_des_path}/{image_name}.pkl'
            with open(des_file, "rb") as c:
                des_feats = pickle.load(c)
            dataset_dict["des_feats"] = des_feats
            dataset_dict['transforms'] = None
        # USER: Remove if you don't do semantic/panoptic segmentation.
        if "sem_seg_file_name" in dataset_dict:
            sem_seg_gt = utils.read_image(dataset_dict.pop("sem_seg_file_name"), "L").squeeze(2)
        else:
            sem_seg_gt = None

        aug_input = T.AugInput(image, sem_seg=sem_seg_gt)
        transforms = self.augmentations(aug_input)
        image, sem_seg_gt = aug_input.image, aug_input.sem_seg

        image_shape = image.shape[:2]  # h, w
        # Pytorch's dataloader is efficient on torch.Tensor due to shared-memory,
        # but not efficient on large generic data structures due to the use of pickle & mp.Queue.
        # Therefore it's important to use torch.Tensor.
        dataset_dict["image"] = torch.as_tensor(np.ascontiguousarray(image.transpose(2, 0, 1)))
        if sem_seg_gt is not None:
            dataset_dict["sem_seg"] = torch.as_tensor(sem_seg_gt.astype("long"))

        # USER: Remove if you don't use pre-computed proposals.
        # Most users would not need this feature.
        if self.proposal_topk is not None:
            utils.transform_proposals(
                dataset_dict, image_shape, transforms, proposal_topk=self.proposal_topk
            )

        if not self.is_train:
            # USER: Modify this if you want to keep them for some reason.
            dataset_dict.pop("annotations", None)
            dataset_dict.pop("sem_seg_file_name", None)
            return dataset_dict

        if "annotations" in dataset_dict:
            self._transform_annotations(dataset_dict, transforms, image_shape)

        return dataset_dict
